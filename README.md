## Framing the Problem

Our prediction problem is seeing whether or not we can accurately predict whether a team loses or wins based on certain features of the game: game length, team side, team kills, team deaths, and team kills per minute. For this problem, we are building a binary classifier, because there are only 2 options for the result: win and loss. We chose the game result as the response variable because we believe that all the variables that we chose have an effect on the result of the match and that predicting the result of the match will be very important to the game as teams and players will be able to implement the model and use it to increase their chances of winning. We chose these certain features of the game to predict the result of the game because these variables are all team variables, while all the other variables in the dataset are related to individual data, and so we wanted to solely use team variables to predict the team result. As our metric to evaluate our model, we will be using the accuracy of our model. We chose this as our metric because as it is a classification model, we want to be able to accurately judge how often our model is getting the right option so that we know whether or not the model accurately reflects the true result and whether or not it is reliable and can be generalized to unseen data.

## Baseline Model

For our baseline model, we used a K Nearest Neighbors. For the features in our baseline model, we used team side and game duration to predict our response variable which is the result of the game. The game duration column is a discrete, quantitative column and so we decided to standardize the column by using StandardScaler in order to ensure that all the values in the column are standardized and not skewing the distribution. The team side column is nominal, and so we decided to use a LabelBinarizer object to binarize the column so that we could fit the model. After creating a ColumnTransformer to perform the necessary transformations, we then used a K Nearest Neighbor algorithm on it and set the cluster size to 3. We then calculated the accuracy of the model and found that the model achieved a 49.73% accuracy. In our opinion, the baseline model is not good enough because it has very low accuracy (more than ½ of the data is misclassified which is worse than chance). In addition, it only has 2 features and because we don’t know if those features are actually the most important ones in classifying whether or not a team is going to win or lose, the baseline model has shortcomings that were addressed in our final model.

<iframe src="assets/teamkills.html" width=800 height=600 frameBorder=0></iframe>

## Final Model

To find out what features were the most important in classifying whether or not a team will win or lose and ensuring that we can maximize the accuracy of the predictions of our model, we used a random forest classifier and trained it on the datasets, and then used the feature_importances_ attribute to discover what the most important features were to our prediction problem so that we could implement them in our final model. A random forest classifier was a good choice for feature selection because it automatically calculates feature weights and importance at the end of the algorithm, so we leveraged that to be able to tell us what features we needed to include in our model before we actually fit and trained our model on the data. We found out that team deaths, team kills per minute, team kills, and team side were the most important features. This made sense to us because it seemed that out of all the variables, game duration would affect the result of the game the least. Therefore, we used these features and excluded the game length variable from our model, which was originally included in our dataset. We then performed a grid search on the most optimal combination of hyperparameters to fit into our Random Forest classification model. We chose this to test in the grid search and be our final model because it can handle large datasets well, it is not prone to overfitting, and it in theory should be more accurate than the K-Nearest Neighbors algorithm because of its use of ensemble learning, which uses multiple instances of the classifier to make the final decision instead of just one. In our grid search, we tested values for the number of trees in the forest, the criterion for error, and the maximum depth a tree is allowed to reach. We found out that the best hyperparameters for our model was the gini criterion with a max depth of 2 and with 100 trees in the forest. We then used those parameters and the features that we found to train our model and fit it to a random forest classifier. After training it, we found that the accuracy was 92.31%, which is a huge improvement from our baseline model because it was almost double the accuracy. Because it performed better on the test data than the baseline model, it should also generalize better to more unseen data if it were to be fed into our model, and be more accurate in classifying that as well.

## Fairness Analysis

We wanted to see whether or not the number of team kills affects the performance of our model. We split the team kills column in our dataset into 2 groups: one group that contained teams that had less than or equal to 10 kills, and another group that contained teams that had more than 10 kills, and then binarized the two groups where the first group was represented by a 0 and the second group was represented by a 1. We decided to use a permutation test to see whether or not the two groups belonged to the same population or not. Our null hypothesis for this test was that the model was fair and that there was no significant difference in model accuracies between the two groups. Our alternative hypothesis for this test was that the model was not fair and that the number of kills that a team got influenced the model’s ability to accurately predict the match result. As our test statistic, we decided to use the absolute differences in model accuracy between the two groups. For this test, we set a significance level of p = 0.05 , as it seemed a good enough p-value to balance out the chances of making a Type I or Type II error (we would be able to see the real effect while also balancing out the probability that we mistake no effect for a real effect). After finding our test statistic and setting our p-value, we ran 100 iterations of shuffling the team kills columns where we randomly assign each team a number of kills, binarized the columns, fit our model separately for each group, and then calculated the resulting absolute difference in the model accuracies between the two groups. We got a p-value of 0.53, and since it is greater than our p-value, we failed to reject the null hypothesis and we concluded that we cannot say that our model is not fair because there was no significant evidence to show that there was a difference in the way it classified teams that got less or equal to 10 kills and teams that got more than 10 kills.